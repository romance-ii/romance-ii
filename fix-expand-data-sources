#!/bin/sh
cat <<END

  Downloading Knowledgebase Data Sources
 ————————————————————————————————————————

This process will take a long time, and a lot of bandwidth and disc space.

I do mean “a lot.” The current set of data sources pulled is more than 60GiB
of general knowledge data — and most of that is in compressed (gzip or
bzip2) files. At least here in 2016, that's a lot of stuff.

The data sources used to prepare the knowledgebase for the artificial
intelligence and natural language interfaces are very large — several dozen
GiB of data, at least.  This process will not be repeated if the upstream
sources have not changed, though, and will resume if interrupted relatively
nicely.

It would be nice to build a torrent source for the assembled data so that it
can be redistributed without taxing the upstream sources, but that is part
of what we're aiming to do with this.

Some of the downloading (particularly DBpedia) will take place in background
processes and try to saturate your downstream.  Once the other (smaller)
downloads are complete, you'll see progress reports every few minutes until
that completes.  If you must kill it, look at the list of background
processes it's watching to see what else to kill; or 

   pgrep -af wget | grep dbpedia

END
echo "Disc space available before we begin:"
df --output=source,avail -h src/
echo "(Say good-bye to at least 60GiB of that.)"
echo ""
(cd src/dbpedia-2016-04; ./download-dbpedia)&
(cd src/conceptnet5-csv; ./download-conceptnet5-csv)
(cd src/extended-wordnet; ./download-extendedwordnet)
(cd src/nell; ./download-nell)
(cd src/yago; ./download-yago)
(cd src/texai; ./download-texai)
bzip2 -d < src/WordNet-3.0/dict/data.noun.bz2 > src/WordNet-3.0/dict/data.noun

cd src/dbpedia-2016-04; ./wait-for-checksums
